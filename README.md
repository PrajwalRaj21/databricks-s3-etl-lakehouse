# ğŸš€ Databricks ETL Pipeline with Amazon S3 (Lakehouse Architecture)

## ğŸ“Œ Project Overview

This project demonstrates a **job-ready batch ETL pipeline** built using **Databricks, PySpark, Delta Lake, and Amazon S3**. The pipeline ingests raw CSV data from S3, processes it through a **Bronze â†’ Silver â†’ Gold** lakehouse architecture, and produces analytics-ready datasets.


---

## ğŸ— Architecture

```
Local CSV Files
      â†“
Amazon S3 (Raw Zone)
      â†“
Databricks Job (PySpark)
      â†“
Delta Lake (Bronze / Silver / Gold)
      â†“
SQL / BI / Analytics
```

---

## ğŸ›  Tech Stack

* Databricks
* Apache Spark (PySpark)
* Delta Lake
* Amazon S3
* Spark SQL
* Databricks Jobs

---

## ğŸ“‚ Repository Structure

```
Databricks_S3_ETL_Lakehouse/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ etl_orders_customers_products.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ orders.csv
â”‚   â”œâ”€â”€ customers.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ etl_architecture.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“¥ Data Sources

The pipeline ingests the following CSV files stored in Amazon S3:

* `orders.csv`
* `customers.csv`
* `products.csv`

### S3 Folder Layout

```
s3://<your-bucket-name>/raw/
â”œâ”€â”€ orders/orders.csv
â”œâ”€â”€ customers/customers.csv
â””â”€â”€ products/products.csv
```

---

## ğŸ”„ ETL Pipeline Design

### ğŸ¥‰ Bronze Layer (Raw Ingestion)

* Reads CSV files directly from Amazon S3
* Stores raw data as **Delta tables**
* Preserves original schema and values

### ğŸ¥ˆ Silver Layer (Clean & Enriched)

* Applies data quality checks (null handling, schema validation)
* Joins orders with customers and products
* Applies business transformations

### ğŸ¥‡ Gold Layer (Analytics Ready)

* Aggregates business metrics
* Produces reporting-ready datasets
* Exposed via SQL tables for BI tools

---

## âš™ï¸ Automation

* The ETL pipeline is executed using **Databricks Jobs**
* Can be scheduled to run daily or hourly
* Includes retry logic and failure monitoring

---

## ğŸ§ª Example Transformations

* Filtering invalid records
* Joining fact and dimension tables
* Aggregating product-level metrics

---

## ğŸ§  Key Concepts Demonstrated

* Batch ETL pipelines
* Lakehouse architecture
* Incremental data ingestion
* Separation of compute and storage
* SQL + PySpark interoperability

---

## ğŸ“„ Resume Highlights

* Built an automated batch ETL pipeline ingesting CSV data from Amazon S3 into Databricks
* Implemented Bronzeâ€“Silverâ€“Gold lakehouse architecture using Delta Lake
* Performed data cleansing, joins, and aggregations using PySpark
* Automated pipeline execution using Databricks Jobs

---

## ğŸš€ Future Enhancements

* Incremental file ingestion
* Schema evolution handling
* Data quality monitoring tables
* Streaming ingestion using Kafka
* dbt integration on Databricks

---

## ğŸ‘¤ Author

**Prajwal Raj Giri**
Aspiring Data Engineer | Data Science & AI Enthusiast

---

â­ If you find this project useful, consider starring the repository!
